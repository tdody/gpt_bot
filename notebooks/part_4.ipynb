{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65efbc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "201af8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 birds in the dataset:\n",
      "\n",
      "Abbott's babbler, Abbott's booby, Abbott's starling, Abbott's sunbird, Abd al-Kuri sparrow, Abdim's stork, Aberdare cisticola, Aberrant bush warbler, Abert's towhee, Abyssinian catbird\n",
      "There are 10,976 birds in the dataset.\n",
      "\n",
      "The shortest character name has 3 characters.\n",
      "The longest character name has 35 characters.\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"./datasets/birds/birds.csv\"\n",
    "\n",
    "birds = open(DATASET_PATH, \"r\").read().splitlines()\n",
    "\n",
    "print(\"First 10 birds in the dataset:\\n\")\n",
    "print(\", \".join(birds[:10]))\n",
    "print(f\"There are {len(birds):,d} birds in the dataset.\")\n",
    "\n",
    "min_length = map(len, birds)\n",
    "max_length = map(len, birds)\n",
    "print(f\"\\nThe shortest character name has {min(min_length)} characters.\")\n",
    "print(f\"The longest character name has {max(max_length)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f9a66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"\n",
    "    Clean the bird name by:\n",
    "    - Removing leading and trailing whitespaces\n",
    "    - Converting to lowercase\n",
    "    - Removing accents\n",
    "    - Removing special characters\n",
    "    - Replacing spaces with underscores\n",
    "    \"\"\"\n",
    "\n",
    "    name = name.strip().lower()\n",
    "    # replace special characters with a space\n",
    "    name = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in name)\n",
    "    name = name.replace(\"`\", \"_\")  # Remove apostrophes\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    name = unidecode(name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebb73e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 28\n",
      "_, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z\n",
      "\n",
      "Token mapping: {1: 'd', 2: 'p', 3: 'g', 4: 'r', 5: 'z', 6: 'w', 7: 'n', 8: 'u', 9: 'o', 10: 'h', 11: 'i', 12: 'c', 13: 'k', 14: 'b', 15: 't', 16: 's', 17: 'x', 18: 'y', 19: 'j', 20: 'a', 21: 'v', 22: '_', 23: 'l', 24: 'q', 25: '`', 26: 'm', 27: 'f', 28: 'e', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# clean all names in the dataset\n",
    "birds = list(map(clean_name, birds))\n",
    "\n",
    "# create a mapping from tokens to indices\n",
    "unique_tokens = set([c for w in birds for c in w])\n",
    "SPECIAL_TOKEN = \".\"\n",
    "index_to_token = {i: t for i, t in enumerate(unique_tokens, start=1)}\n",
    "token_to_index = {v: k for k, v in index_to_token.items()}\n",
    "index_to_token[0] = SPECIAL_TOKEN\n",
    "token_to_index[SPECIAL_TOKEN] = 0\n",
    "vocab_size = len(unique_tokens) + 1\n",
    "\n",
    "# log information about the tokenization\n",
    "print(f\"Number of unique tokens: {len(unique_tokens)}\")\n",
    "print(\", \".join(sorted(unique_tokens)))\n",
    "print(f\"\\nToken mapping: {index_to_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65287c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "CONTEXT_SIZE = 3\n",
    "N_EMBEDDINGS = 10\n",
    "N_HIDDEN = 64\n",
    "N_TOKEN = len(token_to_index)\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_SET_PORTION = 0.8\n",
    "DEVELOPMENT_SET_PORTION = 0.1\n",
    "TEST_SET_PORTION = 1 - (TRAINING_SET_PORTION + DEVELOPMENT_SET_PORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98a0cff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: torch.Size([172513, 3]) torch.Size([172513])\n",
      "Development set shape: torch.Size([21531, 3]) torch.Size([21531])\n",
      "Test set shape: torch.Size([21461, 3]) torch.Size([21461])\n"
     ]
    }
   ],
   "source": [
    "def build_datasets(words: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Build datasets from a list of words by creating input and target tensors.\n",
    "    \n",
    "    Args:\n",
    "        words (list[str]): List of words to build the datasets from.\n",
    "        \n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Tuple containing the input tensor X and target tensor Y.\n",
    "    \"\"\"\n",
    "    # Create a mapping from tokens to indices\n",
    "    X, Y = [], []\n",
    "    \n",
    "    # Create the context for each character in the words\n",
    "    for w in words:\n",
    "        context = [0] * CONTEXT_SIZE\n",
    "        for ch in w + SPECIAL_TOKEN:  # Add special token at the end\n",
    "            ix = token_to_index[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # Update the context by shifting it and adding the new index \n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    X = torch.tensor(X, dtype=torch.int64)\n",
    "    Y = torch.tensor(Y, dtype=torch.int64)\n",
    "\n",
    "    return X, Y\n",
    "    \n",
    "# Shuffle the words\n",
    "random.seed(1234)\n",
    "random.shuffle(birds)\n",
    "\n",
    "# Split the dataset into training, development, and test sets\n",
    "train_size = int(TRAINING_SET_PORTION * len(birds))\n",
    "dev_size = int(DEVELOPMENT_SET_PORTION * len(birds))\n",
    "\n",
    "X_train, Y_train = build_datasets(birds[:train_size])\n",
    "X_dev, Y_dev = build_datasets(birds[train_size:train_size + dev_size])\n",
    "X_test, Y_test = build_datasets(birds[train_size + dev_size:])\n",
    "\n",
    "# print tensor shapes\n",
    "print(\"Training set shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Development set shape:\", X_dev.shape, Y_dev.shape)\n",
    "print(\"Test set shape:\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f0f0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    \"\"\"\n",
    "    Compare the true gradient dt and the approximate gradient t.grad.\n",
    "    Print the results in a table format.\n",
    "    \"\"\"\n",
    "    assert t.grad.shape == dt.shape, f\"Shape mismatch: expected {t.grad.shape}, got {dt.shape}\"\n",
    "\n",
    "    exact = torch.all(dt==t.grad).item()\n",
    "    approx = torch.allclose(dt, t.grad)\n",
    "    max_diff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(exact):5s} | approximate: {str(approx):5s} | maxdiff: {max_diff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e74b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4,287\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(123456789) # for reproducibility\n",
    "\n",
    "# Embedding matrix\n",
    "C = torch.randn((N_TOKEN, N_EMBEDDINGS), generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((N_EMBEDDINGS * CONTEXT_SIZE, N_HIDDEN), generator=g) * (5/3) / (N_EMBEDDINGS * CONTEXT_SIZE)**0.5\n",
    "b1 = torch.randn(N_HIDDEN, generator=g) * 0.1\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((N_HIDDEN, N_TOKEN), generator=g) * 0.1\n",
    "b2 = torch.randn(N_TOKEN, generator=g) * 0.1\n",
    "\n",
    "# Batch normalization\n",
    "bngain = torch.randn((1, N_HIDDEN), generator=g) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, N_HIDDEN), generator=g) * 0.1\n",
    "\n",
    "# Parameters\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "# Model size\n",
    "print(f\"Model size: {sum(p.numel() for p in parameters):,}\")\n",
    "\n",
    "# Turn on gradient tracking\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a60d749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "ix = torch.randint(0, X_train.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = X_train[ix], Y_train[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bafa5d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4834, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass on a single batch\n",
    "n = batch_size\n",
    "emb = C[Xb]                                            # shape (batch_size, context_size, embedding_size)\n",
    "embcat = emb.view(emb.shape[0], -1)                    # shape (batch_size, context_size * embedding_size)\n",
    "\n",
    "# Layer 1\n",
    "hprebn = embcat @ W1 + b1                              # shape (batch_size, hidden_size)\n",
    "\n",
    "# Batch normalization\n",
    "bnmeani = hprebn.sum(dim=0, keepdim=True) / n          # shape (1, hidden_size)\n",
    "bndiff = hprebn - bnmeani                              # shape (batch_size, hidden_size)\n",
    "bndiff2 = bndiff ** 2                                  # shape (batch_size, hidden_size)\n",
    "bnvar = bndiff2.sum(dim=0, keepdim=True) / (n - 1)     # shape (1, hidden_size)\n",
    "bnvar_inv = 1 / torch.sqrt(bnvar + 1e-5)               # shape (1, hidden_size)\n",
    "bnraw = bndiff * bnvar_inv                             # shape (batch_size, hidden_size)\n",
    "hpreact = bngain * bnraw + bnbias                      # shape (batch_size, hidden_size)\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)                                # shape (batch_size, hidden_size)\n",
    "logits = h @ W2 + b2                                   # shape (batch_size, vocab_size)\n",
    "\n",
    "# Cross-entropy loss\n",
    "logit_maxes = logits.max(dim=1, keepdim=True).values   # shape (batch_size, 1)\n",
    "norm_logits = logits - logit_maxes                     # shape (batch_size, vocab_size)\n",
    "counts = norm_logits.exp()                             # shape (batch_size, vocab_size)\n",
    "counts_sum = counts.sum(dim=1, keepdim=True)           # shape (batch_size, 1)\n",
    "counts_sum_inv = counts_sum ** -1                      # shape (batch_size, 1)\n",
    "probs = counts * counts_sum_inv                        # shape (batch_size, vocab_size)\n",
    "logprobs = probs.log()                                 # shape (batch_size, vocab_size)\n",
    "loss = - logprobs[range(logprobs.shape[0]), Yb].mean() # shape (1)  \n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [\n",
    "    logprobs, probs, counts_sum_inv, counts_sum, counts,\n",
    "    norm_logits, logit_maxes, logits, h, hpreact,\n",
    "    bnraw, bnvar_inv, bnvar, bndiff, bndiff2,\n",
    "    bnmeani, hprebn, embcat, emb\n",
    "    ]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69d8d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all the gradients manually and compare with PyTorch backward pass\n",
    "dlogprobs = torch.zeros_like(logprobs).index_put((torch.Tensor(range(n)).int(), Yb), torch.tensor(-1/n))        # shape (batch_size, vocab_size)\n",
    "dprobs = dlogprobs * (1 / probs)                                                                                # shape (batch_size, vocab_size)\n",
    "dcounts_sum_inv = (dprobs * counts).sum(dim=1, keepdim=True)                                                    # shape (batch_size, 1)\n",
    "dcounts_sum = dcounts_sum_inv * ( - 1 / counts_sum ** 2 )                                                       # shape (batch_size, 1)\n",
    "dcounts = counts_sum_inv * dprobs + torch.ones_like(counts) * dcounts_sum                                       # shape (batch_size, vocab_size)\n",
    "dnorm_logits = dcounts * counts                                                                                 # shape (batch_size, vocab_size)\n",
    "dlogit_maxes = (-dnorm_logits).sum(dim=1, keepdim=True)                                                         # shape (batch_size, 1)\n",
    "dlogits = dlogit_maxes * F.one_hot(logits.max (1).indices, num_classes=logits.shape [1]) + dnorm_logits.clone() # shape (batch_size, vocab_size)\n",
    "dh = dlogits @ W2.T                                                                                             # shape (batch_size, hidden_size)\n",
    "dW2 = h.T @ dlogits                                                                                             # shape (hidden_size, hidden_size)\n",
    "db2 = dlogits.sum(dim=0, keepdim=False)                                                                         # shape (hidden_size)\n",
    "dhpreact = dh * (1 - h ** 2)                                                                                    # shape (batch_size, hidden_size)\n",
    "dbngain = (dhpreact * bnraw).sum(dim=0, keepdim=True)                                                           # shape (1, hidden_size)\n",
    "dbnbias = dhpreact.sum(dim=0, keepdim=True)                                                                     # shape (1, hidden_size)\n",
    "dbnraw = (dhpreact * bngain)                                                                                    # shape (batch_size, hidden_size)\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)                                                             # shape (1, hidden_size)\n",
    "dbnvar = (-0.5*(bnvar + 1.0e-5)**-1.5) * dbnvar_inv                                                             # shape (1, hidden_size)\n",
    "# Using tbe bnvar grad instead of the manually computed value to prevent the numerical rounding to compound\n",
    "dbndiff2 = bnvar.grad * torch.ones_like(bndiff2) / (n-1)                                                        # shape (batch_size, hidden_size)\n",
    "dbndiff = dbndiff2 * 2 * bndiff + dbnraw * bnvar_inv                                                            # shape (batch_size, hidden_size)\n",
    "dbnmeani = - dbndiff.sum(0, keepdim=True)                                                                       # shape (batch_size, 1)\n",
    "dhprebn = dbndiff + dbnmeani * (1/n)                                                                            # shape (batch_size, hidden_size)\n",
    "dembcat = dhprebn @ W1.T                                                                                        # shape (batch_size, vocab_size)\n",
    "dW1 = embcat.T @ dhprebn                                                                                        # shape (hidden_size, vocab_size)\n",
    "db1 = dhprebn.sum(dim=0, keepdim=False)                                                                         # shape (hidden_size)\n",
    "demb = dembcat.view(emb.shape)                                                                                  # shape (vocab_size)\n",
    "dC = torch.zeros_like(C)                                                                                        # shape (vocab_size, N_EMBEDDINGS)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k, j]\n",
    "        dC[ix] += demb[k, j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "926807cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bacddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss fast: 3.48343825340271, diff: 0.00000%\n"
     ]
    }
   ],
   "source": [
    "# backprop through cross_entropy but all in one go\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(f\"Loss fast: {loss_fast.item()}, diff: {(loss_fast - loss) / loss:.5%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6775ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 5.122274160385132e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39d7cf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAHiCAYAAACA1URdAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASkZJREFUeJzt3Xt0VfWd//9XAjkJkFsDSU4yhJsXCCpQg4TTWqskJYBtdUhniWXNgE1h7DdxRkLHSseKtuMwtRe8FGT1q4X2K1RLl9YRHDSC4LcaQGMZviBEwSAROAGluRDIBXJ+f/jLGY9JYH8+5uxk4/Ox1l6afT6fvT/7nH3CO+/92fsdEwqFQgIAAHAgtq8HAAAAvIPAAQAAOEbgAAAAHCNwAAAAjhE4AAAAxwgcAACAYwQOAADAMQIHAADg2MC+HgAAALZaWlrU1tYWlW37fD4lJCREZdteRuAAAPCklpYWjR49WsFgMCrb9/v9qqmpIXj4FAIHAIAntbW1KRgMqra2VsnJyb267cbGRuXk5KitrY3A4VMIHAAAnpaUlKSkpKRe3SZlnHrG5EgAAOAYGQcAgKeFQqFezxCQcegZgQMAwNMIHNzFpQoAAOAYGQcAgKeRcXAXGQcAAOAYGQcAgKeRcXAXGQcAAOAYGQcAgKeRcXAXGQcAAOAYGQcAgKeRcXAXgQMAwNMIHNzFpQoAAOAYGQcAgKeRcXAXGQcAAOAYGQcAgKeRcXAXGQcAAOAYGQcAgKeRcXAXGQcAAOAYGQcAgKeRcXAXgQMAwNMIHNzFpQoAAOBYv8s4dHR06OjRo0pKSlJMTExfDwcAYCkUCqmpqUnZ2dmKjY3e36lkHNzV7wKHo0ePKicnp6+HAQDoJbW1tRo+fHhfDwO9pN8FDklJSZKknTt3KjEx0XG/AQMGGO/r3LlzRu07OjqM92HDJtNic/w+n8+4T3Nzs1H7L37xi8b72Lt3r3Ef089SsnvP4uPjjfu0tLQYtbcZl43W1lbjPoMGDTLuY/O9aWtrM+5jKiEhwbhPe3u7cR+bz9PmszFl8vu1k+nxNzU16eqrrw7/Xo8WMg7uilrgsGLFCv3sZz9TMBjUxIkT9eijj2rKlCkX7Nf5j2ZiYqLRyUbg4E7gYJputDkWm18ybgUONv/YxMXFGbV3K3Cw+fwJHNwJHGw+G1M23zPbz4XLzheXqFx0evrpp1VeXq6lS5fqrbfe0sSJE1VUVKTjx49HY3cAgM+xzoxDby/oXlQCh1/+8pdasGCBbrvtNo0fP16rVq3S4MGD9Zvf/CYauwMAAC7p9UsVbW1tqqqq0pIlS8LrYmNjVVhYqMrKyi7tW1tbI67nNTY29vaQAAAXMeY4uKvXMw4ffvihzp07p8zMzIj1mZmZCgaDXdovW7ZMKSkp4YU7KgAAJrhU4a4+fwDUkiVL1NDQEF5qa2v7ekgAAKAHvX6pYtiwYRowYIDq6uoi1tfV1cnv93dpHx8fb3WLGwAAncgQuKfXMw4+n095eXnavHlzeF1HR4c2b96sQCDQ27sDAAAuispzHMrLyzVv3jxNnjxZU6ZM0UMPPaTm5mbddttt0dgdAOBzjMmR7opK4HDLLbfoxIkTuvfeexUMBjVp0iRt2rSpy4RJAADgLVF7cmRZWZnKysqs+/t8PqOnp7nxpDmbp+bZRK1uHIsknT592riP6VPw3nvvPeN92DwF8pJLLjHu88477xj3sfk8TZ8cafO4YZunE9qczzbcOp9N2TzN0GY+ls3xm+5n4EDzX+Wmj4+XzL+bpo9bt0XGwV19flcFAADwjn5X5AoAABNkHNxFxgEAADhGxgEA4GlkHNxF4AAA8DQCB3dxqQIAADhGxgEA4GlkHNxFxgEAADhGxgEA4GlkHNxFxgEAADhGxgEA4GlkHNxFxgEAADjWbzMOphGkTZGjgwcPGrVPTk423seJEyeM+7gV6SYmJhr3MS2MFRtrHpuePXvWuM+BAweM+9gUObLR3t7uyn5MnTlzxriPSeG5Tjbns+l5Y7MPtwowdXR0GPcx/W7aFKyzec/S0tKM2tsU37JBxsFd/TZwAADACQIHd3GpAgAAOEbGAQDgaWQc3EXGAQAAOEbgAADwtM6MQ28vplasWKFRo0YpISFB+fn52rlz53nbr1+/XuPGjVNCQoKuuuoqvfDCC12O695771VWVpYGDRqkwsJCvfvuuxFtTp48qblz5yo5OVmpqakqKSnRqVOnjMdugsABAIDP6Omnn1Z5ebmWLl2qt956SxMnTlRRUZGOHz/ebfvXX39dt956q0pKSvSXv/xFN998s26++Wbt2bMn3ObBBx/UI488olWrVmnHjh0aMmSIioqKIu4Imjt3rvbu3auKigpt2LBBr776qhYuXBjVY40J9bMLOY2NjUpJSdG7776rpKQkx/1GjBhhvC/T2zHT09ON9+HW7ZgDBgww7pOQkGDcp7/ejmlza6VNH5vjMWVz+57N529zm6jN7ZhtbW3Gfdy4HdOt23EvptsxU1NTjdo3NjZq+PDhamhosLqd3cn2U1JStHv3bqN/L5xoamrShAkTHI89Pz9f11xzjX71q19J+vhzz8nJ0R133KG77767S/tbbrlFzc3N2rBhQ3jd1KlTNWnSJK1atUqhUEjZ2dlavHixvv/970uSGhoalJmZqTVr1mjOnDnat2+fxo8frzfeeEOTJ0+WJG3atEmzZs3SBx98oOzs7N54K7og4wAAwGfQ1tamqqoqFRYWhtfFxsaqsLBQlZWV3faprKyMaC9JRUVF4fY1NTUKBoMRbVJSUpSfnx9uU1lZqdTU1HDQIEmFhYWKjY3Vjh07eu34Po27KgAAnhbNuyoaGxsj1sfHxys+Pj5i3Ycffqhz584pMzMzYn1mZqb279/f7faDwWC37YPBYPj1znXna5ORkRHx+sCBA5WWlhZuEw1kHAAAnhbNyZE5OTlKSUkJL8uWLevjo+17ZBwAAOhBbW1txByHT2cbJGnYsGEaMGCA6urqItbX1dXJ7/d3u12/33/e9p3/raurU1ZWVkSbSZMmhdt8evLl2bNndfLkyR732xvIOAAAPC2aGYfk5OSIpbvAwefzKS8vT5s3bw6v6+jo0ObNmxUIBLodcyAQiGgvSRUVFeH2o0ePlt/vj2jT2NioHTt2hNsEAgHV19erqqoq3GbLli3q6OhQfn6+5bt5Yf0243Du3DmdO3fOcXvTOySkjyeamPh0dBgtbt2J0NzcbNzHdCa6zUznv/71r8Z9bK5v2hTgMTknbftcccUVxvt4++23jfu49Z4NGTIk6vv58MMPjfdhw+a7afM+m94l4dZdNabfzaamJuN9eFV5ebnmzZunyZMna8qUKXrooYfU3Nys2267TZL0D//wD/qbv/mb8KWOf/7nf9ZXv/pV/eIXv9CNN96op556Sm+++aZ+/etfS/r4d+2dd96pf/u3f9Nll12m0aNH60c/+pGys7N18803S5Jyc3M1Y8YMLViwQKtWrVJ7e7vKyso0Z86cqN1RIfXjwAEAACf6wyOnb7nlFp04cUL33nuvgsGgJk2apE2bNoUnNx4+fDgi8PzSl76kdevW6Z577tEPf/hDXXbZZfrTn/6kK6+8MtzmrrvuUnNzsxYuXKj6+npde+212rRpU8St9GvXrlVZWZkKCgoUGxur4uJiPfLII5/x6M+v3z7HYf/+/UZ/rdrcK22acbD5S9iGzV81NsfvxrMPTN9jyb33OS4uzrjP5z3jYJM96C61eyFuZBzceo6HzTljOja3Mg6mmpqadOWVV0b9OQ5vvfVWVJ7jcPXVV0dt7F5GxgEA4Hn97G/gixqTIwEAgGNkHAAAntYf5jh8nhA4AAA8jcDBXVyqAAAAjpFxAAB4GhkHd5FxAAAAjpFxAAB4GhkHd5FxAAAAjvXbjEN3Nc/Pp7W11Xgfpk8o/ORjPp2yGZfNfmyeTmcTUZs+bdHmKZA2T+dzi83YTJ+cuH//fuN9uPW0UZunDba0tBj3MeXWUyDdeNqqZD62MWPGGO/j3XffNe5j+oRKm6ez2iDj4K7++xsaAAD0O/024wAAgBNkHNxF4AAA8DQCB3dxqQIAADhGxgEA4GlkHNxFxgEAADhGxgEA4GlkHNxFxgEAADhGxgEA4GlkHNxFxgEAADhGxgEA4GlkHNxF4AAA8DQCB3f128DhzJkzGjgwusP7whe+YNS+oaHBeB82BW5Onz5t3MemYI9Nn+bmZqP2Pp/PeB82X1ibIl82fZKSkoz7NDY2GrW3OWds3mebIlc2bI7n7NmzURhJJJv3rK2tzbiPzfls+p7ZFKyy+f4PHz7cqD3/+F6c+m3gAACAE2Qc3MXkSAAA4FivBw733XefYmJiIpZx48b19m4AAJD0PxmH3l7Qvahcqrjiiiv08ssv/89OojxXAQAAuCMq/6IPHDhQfr8/GpsGACACcxzcFZU5Du+++66ys7M1ZswYzZ07V4cPH+6xbWtrqxobGyMWAADQP/V64JCfn681a9Zo06ZNeuyxx1RTU6OvfOUrampq6rb9smXLlJKSEl5ycnJ6e0gAgIsYcxzc1euBw8yZM/V3f/d3mjBhgoqKivTCCy+ovr5ef/jDH7ptv2TJEjU0NISX2tra3h4SAOAiRuDgrqjPWkxNTdXll1+uAwcOdPt6fHy84uPjoz0MAADQC6L+HIdTp07p4MGDysrKivauAACfU2Qb3NPrgcP3v/99bdu2TYcOHdLrr7+uv/3bv9WAAQN066239vauAACAy3r9UsUHH3ygW2+9VR999JHS09N17bXXavv27UpPT+/tXQEAwO2YLuv1wOGpp57qle3ExcUpLi7OcXubgj2mRatsisLYPPzKrYJNNn3GjBlj1D4YDBrv48yZM8Z9BgwYYNzHppCSaZEvSUpMTDRq71aRL5uCTTbfAZPvcSfTIk8tLS3G+7D5/G2+zza/m9wo8mWzj5qaGqP2TU1NuvLKK433g/6NRzoCADyNjIO7KHIFAAAcI+MAAPA0Mg7uInAAAHgagYO7uFQBAAAcI+MAAPA0Mg7uIuMAAAAcI+MAAPA0Mg7uIuMAAAAcI+MAAPA0Mg7uIuMAAAAcI+MAAPA0Mg7uumgCB5sPOSEhwai9TfElG6bjkqTTp09HYSRdHTp0yKi9TSGloUOHGvepr6837mNTsMmmmNSpU6eM+5iyKfI1aNAg4z6tra3Gfdwo2OTz+Yz75OTkGPcxLfJky/T3WXx8vPE+TAuJSeafv80+bBA4uItLFQAAwLGLJuMAAPh8IuPgLjIOAADAMTIOAABPI+PgLjIOAADAMQIHAICndWYcenuJlpMnT2ru3LlKTk5WamqqSkpKLnj3VUtLi0pLSzV06FAlJiaquLhYdXV1EW0OHz6sG2+8UYMHD1ZGRob+5V/+JeKupq1btyomJqbLEgwGjcZP4AAAgIvmzp2rvXv3qqKiQhs2bNCrr76qhQsXnrfPokWL9Pzzz2v9+vXatm2bjh49qtmzZ4dfP3funG688Ua1tbXp9ddf129/+1utWbNG9957b5dtVVdX69ixY+ElIyPDaPzMcQAAeJqX5jjs27dPmzZt0htvvKHJkydLkh599FHNmjVLP//5z5Wdnd2lT0NDg5544gmtW7dO06ZNkyStXr1aubm52r59u6ZOnaqXXnpJb7/9tl5++WVlZmZq0qRJ+slPfqIf/OAHuu+++yKedZKRkaHU1FTrYyDjAACASyorK5WamhoOGiSpsLBQsbGx2rFjR7d9qqqq1N7ersLCwvC6cePGacSIEaqsrAxv96qrrlJmZma4TVFRkRobG7V3796I7U2aNElZWVn62te+ptdee834GMg4AAA8LZoZh8bGxoj18fHxVk/q7BQMBrtcGhg4cKDS0tJ6nGsQDAbl8/m6ZAkyMzPDfYLBYETQ0Pl652uSlJWVpVWrVmny5MlqbW3V448/ruuvv147duzQ1Vdf7fgYyDgAADwtmpMjc3JylJKSEl6WLVvW7RjuvvvubicefnLZv3+/m29LF2PHjtU//uM/Ki8vT1/60pf0m9/8Rl/60pe0fPlyo+2QcQAAoAe1tbVKTk4O/9xTtmHx4sWaP3/+ebc1ZswY+f1+HT9+PGL92bNndfLkSfn9/m77+f1+tbW1qb6+PiLrUFdXF+7j9/u1c+fOiH6dd130tF1JmjJliv785z+fd9yf1m8Dh7a2NqMCSXFxccb7aG5uNmpvU0jHpsBPR0eHcR+bIkduPODEpsjNyZMnozCSrmzSjTZFnkyLaQ0caP61tDnPbAqjuVW0KCkpyah9Q0OD8T5svmc2bH5vmI7NZh9NTU3GfUz3Y/N72UY0L1UkJydHBA49SU9PV3p6+gXbBQIB1dfXq6qqSnl5eZKkLVu2qKOjQ/n5+d32ycvLU1xcnDZv3qzi4mJJH98ZcfjwYQUCgfB2H3jgAR0/fjx8KaSiokLJyckaP358j+PZtWuXsrKyLjjuT+q3gQMAABeb3NxczZgxQwsWLNCqVavU3t6usrIyzZkzJ3xHxZEjR1RQUKDf/e53mjJlilJSUlRSUqLy8nKlpaUpOTlZd9xxhwKBgKZOnSpJmj59usaPH6+///u/14MPPqhgMKh77rlHpaWl4T+SHnroIY0ePVpXXHGFWlpa9Pjjj2vLli166aWXjI6BwAEA4Gleuh1TktauXauysjIVFBQoNjZWxcXFeuSRR8Kvt7e3q7q6OiIruHz58nDb1tZWFRUVaeXKleHXBwwYoA0bNuh73/ueAoGAhgwZonnz5unHP/5xuE1bW5sWL16sI0eOaPDgwZowYYJefvll3XDDDUbjjwn1swdyNzY2KiUlRXv37jVKV9qkxEzTzm5dqhg0aJBxn/b2duM+/fVSxblz56Iwkq4+75cqbN7ni+lSxejRo437vP/++8Z93LhUYXMuu3GpoqmpSZdffrkaGhocpftNdf578cc//lFDhgzp1W03NzfrW9/6VtTG7mVkHAAAntfP/ga+qHE7JgAAcIyMAwDA07w2x8HrCBwAAJ5G4OAuLlUAAADHyDgAADyNjIO7yDgAAADHyDgAADyNjIO7yDgAAADH+m3GYcKECUZPqTt8+LDxPhISEozamxTd6mTzFECb4kM2T6ezKYxl+uREt540aHMsLS0txn2+8IUvGPepr683au9WYTQbgwcPNu5j81RT06ca2nzPjh07ZtzHhs3vDdO/dm32YfOEWtPvv825bIOMg7vIOAAAAMf6bcYBAAAnyDi4i8ABAOBpBA7u4lIFAABwjIwDAMDTyDi4i4wDAABwjIwDAMDTyDi4i4wDAABwjIwDAMDTyDi4i4wDAABwjIwDAMDTyDi4q98GDrt371ZSUpLj9jYfss0z9N3Yh009hIaGBuM+Nkzf5/j4eON92NS3yMnJMe5z9OhR4z6mdSck8/fMpu6ETa0S07oDknTq1CnjPjb1LUzfA5vvWWysecLVpvaCzWdjWkfFplaHDdOaMDY1ZGwQOLiLSxUAAMCxfptxAADACTIO7jLOOLz66qv6xje+oezsbMXExOhPf/pTxOuhUEj33nuvsrKyNGjQIBUWFurdd9/trfECAIA+ZBw4NDc3a+LEiVqxYkW3rz/44IN65JFHtGrVKu3YsUNDhgxRUVGR8TU7AACc6Mw49PaC7hlfqpg5c6ZmzpzZ7WuhUEgPPfSQ7rnnHt10002SpN/97nfKzMzUn/70J82ZM+ezjRYAAPSpXp0cWVNTo2AwqMLCwvC6lJQU5efnq7Kysts+ra2tamxsjFgAAHCKjIO7ejVwCAaDkqTMzMyI9ZmZmeHXPm3ZsmVKSUkJLza31QEAAHf0+e2YS5YsUUNDQ3ipra3t6yEBADyEjIO7evV2TL/fL0mqq6tTVlZWeH1dXZ0mTZrUbZ/4+HirhwQBACBxO6bbejXjMHr0aPn9fm3evDm8rrGxUTt27FAgEOjNXQEAgD5gnHE4deqUDhw4EP65pqZGu3btUlpamkaMGKE777xT//Zv/6bLLrtMo0eP1o9+9CNlZ2fr5ptv7s1xAwAgiYyD24wDhzfffFM33HBD+Ofy8nJJ0rx587RmzRrdddddam5u1sKFC1VfX69rr71WmzZtUkJCQu+NGgAA9AnjwOH6668/byQWExOjH//4x/rxj3/8mQYWFxenuLg4x+1tCgOZzq2wKQpkU3zmzJkzxn3cio5NPhNJGjFihPE+PpnRcurQoUPGfWwKI9mcZ6Z93DpnbIo82fSxec9SUlKM2tsU37IZl422tjbjPqbv86BBg4z3YfNQPtPvjBuFBDuRIXBPn99VAQAAvIMiVwAAT2OOg7vIOAAAAMfIOAAAPI2Mg7sIHAAAnkbg4C4uVQAAAMfIOAAAPI2Mg7vIOAAAAMfIOAAAPI2Mg7vIOAAAAMfIOAAAPI2Mg7vIOAAAAMcumoyDTfEdm6JVps6ePWvcx6bIkU31UZvjNy1aY1OwyrT4mGRXsOjcuXPGfWw+G9PCYDafy+DBg437mI5LsismZVPkKSkpyaj9+++/b7yPAQMGGPexYfM+m54DNkXObL4zpr9nbX4v2yDj4K6LJnAAAHw+ETi4i0sVAADAMTIOAABPI+PgLjIOAADAMTIOAABPI+PgLjIOAADAMQIHAICndWYcenuJlpMnT2ru3LlKTk5WamqqSkpKLnirc0tLi0pLSzV06FAlJiaquLhYdXV1EW3+6Z/+SXl5eYqPj9ekSZO63c7u3bv1la98RQkJCcrJydGDDz5oPH4CBwAAXDR37lzt3btXFRUV2rBhg1599VUtXLjwvH0WLVqk559/XuvXr9e2bdt09OhRzZ49u0u773znO7rlllu63UZjY6OmT5+ukSNHqqqqSj/72c9033336de//rXR+JnjAADwNC/Ncdi3b582bdqkN954Q5MnT5YkPfroo5o1a5Z+/vOfKzs7u0ufhoYGPfHEE1q3bp2mTZsmSVq9erVyc3O1fft2TZ06VZL0yCOPSJJOnDih3bt3d9nO2rVr1dbWpt/85jfy+Xy64oortGvXLv3yl7+8YODySWQcAACeFs1LFY2NjRHLZ33icGVlpVJTU8NBgyQVFhYqNjZWO3bs6LZPVVWV2tvbVVhYGF43btw4jRgxQpWVlUb7vu666+Tz+cLrioqKVF1drb/+9a+Ot0PgAABAD3JycpSSkhJeli1b9pm2FwwGlZGREbFu4MCBSktLUzAY7LGPz+dTampqxPrMzMwe+/S0nczMzC7b6HzNKS5VAAA8LZqXKmpra5WcnBxe31Mtnbvvvls//elPz7vNffv29d4A+1C/DRxiYmIUExNj1N6UaQEqm+JLNkWubIrP2Hxp3Ci+Y3MsI0eONO7z3nvvGfexGdsnU3xOmb7PLS0txvuwKdjkRpE3ye59rqmpicJIItn8zrB5n22O33RsNsdio78WuYqm5OTkiMChJ4sXL9b8+fPP22bMmDHy+/06fvx4xPqzZ8/q5MmT8vv93fbz+/1qa2tTfX19RNahrq6uxz49befTd2J0/myynX4bOAAA4ER/mByZnp6u9PT0C7YLBAKqr69XVVWV8vLyJElbtmxRR0eH8vPzu+2Tl5enuLg4bd68WcXFxZKk6upqHT58WIFAwPEYA4GA/vVf/1Xt7e3hP2gqKio0duxYfeELX3C8He+HgwAAeERubq5mzJihBQsWaOfOnXrttddUVlamOXPmhO+oOHLkiMaNG6edO3dKklJSUlRSUqLy8nK98sorqqqq0m233aZAIBC+o0KSDhw4oF27dikYDOrMmTPatWuXdu3aFS5t/+1vf1s+n08lJSXau3evnn76aT388MMqLy83OgYyDgAAT+sPGQcTa9euVVlZmQoKChQbG6vi4uLwrZSS1N7erurqap0+fTq8bvny5eG2ra2tKioq0sqVKyO2+93vflfbtm0L//zFL35R0seX/kaNGqWUlBS99NJLKi0tVV5enoYNG6Z7773X6FZMicABAABXpaWlad26dT2+PmrUqC6BS0JCglasWKEVK1b02G/r1q0X3PeECRP0f//v/3U81u4QOAAAPM1rGQevY44DAABwjIwDAMDzyBC4h8ABAOBpXKpwF5cqAACAY2QcAACeRsbBXWQcAACAY2QcAACeRsbBXf02cMjNzTUq3HL48GHjfZgWLDp37pzxPmyK4rS3txv3sSnAZcO0aI1bBY5sPhsbNu9zU1OTUXubc+aTT5hzyqYAkU2Rr87H3UaTzbjcKkDnBpt/5AYONP/1b7oft4pvwV39NnAAAMAJMg7uYo4DAABwjIwDAMDTyDi4i8ABAOBpBA7u4lIFAABwjIwDAMDTyDi4i4wDAABwjIwDAMDTyDi4i4wDAABwjIwDAMDTyDi4i4wDAABwjIwDAMDTyDi4q98GDu+++66Sk5Mdt7cp8hMXF2fU3qb4kE0hGZtjsSny1NraatzH9D2w+fK5VbDH5n02LVhlw6b4kk0xobS0NOM+H374oXEfN34Bu/WeDR482LiPzXlm+j2zKb5mM64RI0YY93EDgYO7uFQBAAAcMw4cXn31VX3jG99Qdna2YmJi9Kc//Sni9fnz5ysmJiZimTFjRm+NFwCACJ0Zh95e0D3jwKG5uVkTJ07UihUremwzY8YMHTt2LLz8/ve//0yDBAAA/YPxheGZM2dq5syZ520THx8vv99vPSgAAJxijoO7ojLHYevWrcrIyNDYsWP1ve99Tx999FE0dgMAAFzW63dVzJgxQ7Nnz9bo0aN18OBB/fCHP9TMmTNVWVnZ7Uzh1tbWiNn9jY2NvT0kAMBFjIyDu3o9cJgzZ074/6+66ipNmDBBl1xyibZu3aqCgoIu7ZctW6b777+/t4cBAACiIOq3Y44ZM0bDhg3TgQMHun19yZIlamhoCC+1tbXRHhIA4CLCXRXuivoDoD744AN99NFHysrK6vb1+Ph4q4eXAAAgcanCbcaBw6lTpyKyBzU1Ndq1a5fS0tKUlpam+++/X8XFxfL7/Tp48KDuuusuXXrppSoqKurVgQMAAPcZBw5vvvmmbrjhhvDP5eXlkqR58+bpscce0+7du/Xb3/5W9fX1ys7O1vTp0/WTn/yErAIAICrIOLjLOHC4/vrrz/uGvvjii59pQJ1CoZA6OjoctzetOyFJsbFmUzxsTiSbehBusXnuvunxDBkyxHgfzc3Nxn1MP0tJ8vl8xn1saoKY1kSwGVdbW5txn+PHjxv3ufzyy437HDp0yLiP6XfN5nOxOWdszk2bmhimtVdOnTplvA+b2jsnT540at/Y2KhRo0YZ7wf9W78tcgUAgFNkCNxDkSsAAOAYGQcAgKcxx8FdZBwAAIBjZBwAAJ5GxsFdBA4AAE8jcHAXlyoAAIBjZBwAAJ5GxsFdZBwAAIBjZBwAAJ5GxsFdZBwAAIBjZBwAAJ5GxsFd/TZwGDp0qJKTkx23P3z4sPE+TIvcDBo0yHgfNsV3bJw9e9a4T3t7u3Ef08JYNsWXbIoPXXLJJcZ9Plkevj+xec9sCinZsHnPbD5P02Jqbv2StykM58Z3ICEhwXgfNt//M2fOGLVvaWkx3gf6v34bOAAA4AQZB3cROAAAPI3AwV1MjgQAAI6RcQAAeBoZB3eRcQAAAI6RcQAAeBoZB3eRcQAAAI4ROAAAPK0z49DbS7ScPHlSc+fOVXJyslJTU1VSUqJTp06dt09LS4tKS0s1dOhQJSYmqri4WHV1dRFt/umf/kl5eXmKj4/XpEmTumzj0KFDiomJ6bJs377daPwEDgAAuGju3Lnau3evKioqtGHDBr366qtauHDhefssWrRIzz//vNavX69t27bp6NGjmj17dpd23/nOd3TLLbecd1svv/yyjh07Fl7y8vKMxs8cBwCAp3lpjsO+ffu0adMmvfHGG5o8ebIk6dFHH9WsWbP085//XNnZ2V36NDQ06IknntC6des0bdo0SdLq1auVm5ur7du3a+rUqZKkRx55RJJ04sQJ7d69u8cxDB06VH6/3/oYyDgAADwtmpcqGhsbIxbTx6F/WmVlpVJTU8NBgyQVFhYqNjZWO3bs6LZPVVWV2tvbVVhYGF43btw4jRgxQpWVlcZj+OY3v6mMjAxde+21+s///E/j/gQOAAD0ICcnRykpKeFl2bJln2l7wWBQGRkZEesGDhyotLQ0BYPBHvv4fD6lpqZGrM/MzOyxT3cSExP1i1/8QuvXr9fGjRt17bXX6uabbzYOHvrtpYq6ujqdPn3acXubQjpxcXFG7W0izY6ODuM+Aweafyw2xbRsCiOZFrmx+VxsxnXo0CFX9mNTsMhUfHy8cR+bgkVusTkHfD6fUXub76ZN0bpRo0YZ93n33XeN+5ieZ4mJicb7sDlnTH/PuFXkL5qXKmprayMKLvb0/bz77rv105/+9Lzb3LdvX+8N0MKwYcNUXl4e/vmaa67R0aNH9bOf/Uzf/OY3HW+n3wYOAAD0teTkZEeVmhcvXqz58+eft82YMWPk9/t1/PjxiPVnz57VyZMne5x34Pf71dbWpvr6+oisQ11d3WeaqyBJ+fn5qqioMOpD4AAA8LT+MDkyPT1d6enpF2wXCARUX1+vqqqq8N0MW7ZsUUdHh/Lz87vtk5eXp7i4OG3evFnFxcWSpOrqah0+fFiBQMBonJ+2a9cuZWVlGfUhcAAAwCW5ubmaMWOGFixYoFWrVqm9vV1lZWWaM2dO+I6KI0eOqKCgQL/73e80ZcoUpaSkqKSkROXl5UpLS1NycrLuuOMOBQKB8B0VknTgwAGdOnVKwWBQZ86c0a5duyRJ48ePl8/n029/+1v5fD598YtflCQ988wz+s1vfqPHH3/c6BgIHAAAntYfMg4m1q5dq7KyMhUUFCg2NlbFxcXhWymlj+efVFdXR8zzW758ebhta2urioqKtHLlyojtfve739W2bdvCP3cGCDU1NeH5OT/5yU/0/vvva+DAgRo3bpyefvppfetb3zIaP4EDAAAuSktL07p163p8fdSoUV0Cl4SEBK1YsUIrVqzosd/WrVvPu9958+Zp3rx5RmPtDoEDAMDTvJZx8DoCBwCA5/EPvXt4ABQAAHCMjAMAwNO4VOEuMg4AAMAxMg4AAE8j4+AuMg4AAMCxfptxMI0gbaJD0z42BatsirzYFN9xqwCXacEim4Jdn7VsbTQlJCQY9zEtJmRTfGjAgAHGfWzOzcGDBxv3sSlylZmZadTepsiZacE26ePH/Jo6e/ascR/TAnwNDQ3G+7Ap8mZ6/rtVfI2Mg7vIOAAAAMf6bcYBAAAnyDi4i4wDAABwjIwDAMDTyDi4i8ABAOBpBA7u4lIFAABwjIwDAMDTyDi4i4wDAABwjIwDAMDTyDi4i4wDAABwjIwDAMDTyDi4i4wDAABwrN9mHGJjY62K45gwLT5jMx6bPk1NTcZ9bIpJ2UTUpoWRbApp2RSSsimMZXP8psWHJPPPpq2tzXgfNkWuUlNTjfucPn3auI/N8bz//vtG7W3OM5ticjk5OcZ93nnnHeM+Pp/PqL1NMSmbgmWmn79bBevIOLir3wYOAAA4QeDgLqM/h5ctW6ZrrrlGSUlJysjI0M0339ylzGxLS4tKS0s1dOhQJSYmqri4WHV1db06aAAA0DeMAodt27aptLRU27dvV0VFhdrb2zV9+nQ1NzeH2yxatEjPP/+81q9fr23btuno0aOaPXt2rw8cAADpfzIOvb2ge0aXKjZt2hTx85o1a5SRkaGqqipdd911amho0BNPPKF169Zp2rRpkqTVq1crNzdX27dv19SpU3tv5AAAwHWfafZhQ0ODJCktLU2SVFVVpfb2dhUWFobbjBs3TiNGjFBlZeVn2RUAAN0i4+Au68mRHR0duvPOO/XlL39ZV155pSQpGAzK5/N1ma2dmZmpYDDY7XZaW1sjZt42NjbaDgkAAESZdcahtLRUe/bs0VNPPfWZBrBs2TKlpKSEF5vbnQAAn19kHNxlFTiUlZVpw4YNeuWVVzR8+PDwer/fr7a2NtXX10e0r6urk9/v73ZbS5YsUUNDQ3ipra21GRIAAHCBUeAQCoVUVlamZ599Vlu2bNHo0aMjXs/Ly1NcXJw2b94cXlddXa3Dhw8rEAh0u834+HglJydHLAAAOEXGwV1GcxxKS0u1bt06Pffcc0pKSgrPW0hJSdGgQYOUkpKikpISlZeXKy0tTcnJybrjjjsUCAS4owIAEBU8AMpdRoHDY489Jkm6/vrrI9avXr1a8+fPlyQtX75csbGxKi4uVmtrq4qKirRy5cpeGSwAAOhbRoGDkwgsISFBK1as0IoVK6wHBQCAU2Qc3NVva1XExMQoJibGqH202RRfyszMNO7z3nvvGfexKfJjU7DJtGiXTfGtU6dOGfexKfJkc87Y/DIxHZvN52Izro8++si4j2nxJcnue3PmzBmj9jbnv825WVNTY9zHtJieZF4YzOb4P/nEX6dMP0ubAmfo//pt4AAAgFNkCNwT3brVAADgokLGAQDgacxxcBcZBwAA4BgZBwCAp5FxcBeBAwDA0wgc3MWlCgAA4BgZBwCAp5FxcBcZBwAA4BgZBwCAp5FxcBcZBwAA4BgZBwCAp5FxcFe/DRzOnj1rVBzGjQ/ZpmCLTcGqwYMHG/cxLQokSefOnTPuY6q9vd24j03BKhumBbsku/fM9HguueQS430cOnTIuI9N8SWbglU2xZRMC5DZfJY2xdTi4+ON+9gw/X1mU3zM5vNvbW2Nant4Q78NHAAAcIKMg7sIHAAAnkbg4C4mRwIAAMfIOAAAPI2Mg7vIOAAAAMfIOAAAPI2Mg7vIOAAA4KKTJ09q7ty5Sk5OVmpqqkpKSi54e3BLS4tKS0s1dOhQJSYmqri4WHV1deHX//u//1u33nqrcnJyNGjQIOXm5urhhx/usp2tW7fq6quvVnx8vC699FKtWbPGePwEDgAAT+vMOPT2Ei1z587V3r17VVFRoQ0bNujVV1/VwoULz9tn0aJFev7557V+/Xpt27ZNR48e1ezZs8OvV1VVKSMjQ08++aT27t2rf/3Xf9WSJUv0q1/9KtympqZGN954o2644Qbt2rVLd955p7773e/qxRdfNBp/TKif5WMaGxuVkpKi/fv3KykpyXE/Nx5mFBcXZ9zH5gFIbj0Ayobpw4xMH+Qj2T2YxobNQ4M6OjqM+5g+nGfUqFHG+7B5AFRLS4txH5PvZCebB0CZsvlcbD5/mwdAufG7yYYb37OmpiaNHz9eDQ0NSk5O7vXtd/57ceONN1r9fj6f9vZ2bdy4sdfHvm/fPo0fP15vvPGGJk+eLEnatGmTZs2apQ8++EDZ2dld+jQ0NCg9PV3r1q3Tt771LUnS/v37lZubq8rKSk2dOrXbfZWWlmrfvn3asmWLJOkHP/iBNm7cqD179oTbzJkzR/X19dq0aZPjYyDjAADwNC9lHCorK5WamhoOGiSpsLBQsbGx2rFjR7d9qqqq1N7ersLCwvC6cePGacSIEaqsrOxxXw0NDUpLS4vY9ye3IUlFRUXn3UZ3mBwJAPC0aE6ObGxsjFgfHx//mR49HgwGlZGREbFu4MCBSktLUzAY7LGPz+dTampqxPrMzMwe+7z++ut6+umntXHjxojtZGZmdtlGY2Ojzpw5o0GDBjk6hn4bOMTGxhqlEm2eoW+aQrS5HGCTDrV5vrtbl1FM63XYHL/Nc/fdSlXbpHdNLwm8//77xvuwOWeGDBli3MemXotN7RHTz8bmXLbhVu0F0/PZJpXe1NRk3GfkyJFG7fvZlXArOTk5ET8vXbpU9913X5d2d999t37605+ed1v79u3rzaH1aM+ePbrpppu0dOlSTZ8+vde3328DBwAAnIhmxqG2tjYiMOsp27B48WLNnz//vNscM2aM/H6/jh8/HrH+7NmzOnnypPx+f7f9/H6/2traVF9fH5F1qKur69Ln7bffVkFBgRYuXKh77rmny3Y+eSdG5zaSk5MdZxskAgcAAHqUnJzsKKOTnp6u9PT0C7YLBAKqr69XVVWV8vLyJElbtmxRR0eH8vPzu+2Tl5enuLg4bd68WcXFxZKk6upqHT58WIFAINxu7969mjZtmubNm6cHHnig232/8MILEesqKioituEEkyMBAJ7mpcmRubm5mjFjhhYsWKCdO3fqtddeU1lZmebMmRO+o+LIkSMaN26cdu7cKUlKSUlRSUmJysvL9corr6iqqkq33XabAoFA+I6KPXv26IYbbtD06dNVXl6uYDCoYDCoEydOhPd9++2367333tNdd92l/fv3a+XKlfrDH/6gRYsWGR0DgQMAAC5au3atxo0bp4KCAs2aNUvXXnutfv3rX4dfb29vV3V1tU6fPh1et3z5cn39619XcXGxrrvuOvn9fj3zzDPh1//4xz/qxIkTevLJJ5WVlRVerrnmmnCb0aNHa+PGjaqoqNDEiRP1i1/8Qo8//riKioqMxt9vn+PwzjvvGN0zbjOhrr9OjrTpYzMBzWZCmel7drFNjnRjcpzJtcZONuOy2Y/N5FA3Phu3Jke65WKbHBnt5zhMnz49Ks9xeOmll6I2di8j4wAAABxjciQAwNMocuUuAgcAgKcROLiLSxUAAMAxMg4AAM8jQ+AeMg4AAMAxMg4AAE9jjoO7+m3gEBMTo5iYGMftTQsJSR9XJDNhc9+/DbcKVtncX2/zvAhTTh7b+mlHjx417mPznrlxDtg8L8SmWp/J96uTzS9T02d/2PYxNWrUKOM+hw8fNu7jxrHYPMfD5nkppsff1NSk8ePHG+8H/Vu/DRwAAHCCjIO7mOMAAAAcI+MAAPA0Mg7uInAAAHgagYO7uFQBAAAcI+MAAPA0Mg7uIuMAAAAcI+MAAPA0Mg7uIuMAAAAcI+MAAPA0Mg7uIuMAAAAcI+MAAPA0Mg7u+lwHDm1tbUbtBw0aZLyP06dPG/exKVhjWrBLsjseUzYFm+rq6qIwkq5sClbZFJMyLSZm8/kPHz7cuE9NTY1xH5siZza/gE0LcNkUbLMpWGVzztiMzfS7aVOwzab41pAhQ6K+DxsEDu7iUgUAAHDMKHBYtmyZrrnmGiUlJSkjI0M333yzqqurI9pcf/314ZLYncvtt9/eq4MGAKBTZ8ahtxd0zyhw2LZtm0pLS7V9+3ZVVFSovb1d06dPV3Nzc0S7BQsW6NixY+HlwQcf7NVBAwCAvmF0YXzTpk0RP69Zs0YZGRmqqqrSddddF14/ePBg+f3+3hkhAADnwRwHd32mOQ4NDQ2SpLS0tIj1a9eu1bBhw3TllVdqyZIlVhMEAQBA/2N9V0VHR4fuvPNOffnLX9aVV14ZXv/tb39bI0eOVHZ2tnbv3q0f/OAHqq6u1jPPPNPtdlpbWyNmkTc2NtoOCQDwOUTGwV3WgUNpaan27NmjP//5zxHrFy5cGP7/q666SllZWSooKNDBgwd1ySWXdNnOsmXLdP/999sOAwAAuMjqUkVZWZk2bNigV1555YL3j+fn50uSDhw40O3rS5YsUUNDQ3ipra21GRIA4HOKuyrcZZRxCIVCuuOOO/Tss89q69atGj169AX77Nq1S5KUlZXV7evx8fFWD9UBAADuMwocSktLtW7dOj333HNKSkpSMBiUJKWkpGjQoEE6ePCg1q1bp1mzZmno0KHavXu3Fi1apOuuu04TJkyIygEAAD7fmOPgLqPA4bHHHpP08UOePmn16tWaP3++fD6fXn75ZT300ENqbm5WTk6OiouLdc899/TagAEA+CQCB3cZX6o4n5ycHG3btu0zDQgAAPRf/bbIVXt7u1HhFp/PZ7wP02JCpkWxJPNiPZJdpHv27FnjPqYFaySpvr7eqL3N52JTsMfmfbYpDGbz2ZgWObIpPnbs2DHjPp9+4qsTNkWebJiem6dOnYrSSCK5dW7a7MeUzfmfmZlp1N6tv9rJOLiLIlcAAMCxfptxAADACTIO7iLjAAAAHCPjAADwPDIE7iHjAAAAHCPjAADwNOY4uIvAAQDgaQQO7uJSBQAAcIyMAwDA08g4uIuMAwAAcIyMAwDA08g4uIuMAwAAcKzfZhwSExOVlJTkuH1LS4vxPkwjSptiNTZFgc6dO2fcx6ZgjU0hnfj4eKP2Z86cMd7HZZddZtzn4MGDxn1sCoPZSE9PN2r/0UcfGe/D5jyLi4sz7mPDtMiX5E7RKpvvmc3vADfYfP42vzPff/99o/ZNTU0aO3as8X5MkXFwFxkHAADgWL/NOAAA4AQZB3cROAAAPI3AwV1cqgAAAI6RcQAAeBoZB3eRcQAAAI6RcQAAeBoZB3eRcQAAwEUnT57U3LlzlZycrNTUVJWUlFzw2SUtLS0qLS3V0KFDlZiYqOLiYtXV1YVf/+///m/deuutysnJ0aBBg5Sbm6uHH344Yhtbt25VTExMlyUYDBqNn4wDAMDTvJZxmDt3ro4dO6aKigq1t7frtttu08KFC7Vu3boe+yxatEgbN27U+vXrlZKSorKyMs2ePVuvvfaaJKmqqkoZGRl68sknlZOTo9dff10LFy7UgAEDVFZWFrGt6upqJScnh3/OyMgwGj+BAwAALtm3b582bdqkN954Q5MnT5YkPfroo5o1a5Z+/vOfKzs7u0ufhoYGPfHEE1q3bp2mTZsmSVq9erVyc3O1fft2TZ06Vd/5znci+owZM0aVlZV65plnugQOGRkZSk1NtT4GLlUAADytM+PQ20s0VFZWKjU1NRw0SFJhYaFiY2O1Y8eObvtUVVWpvb1dhYWF4XXjxo3TiBEjVFlZ2eO+GhoalJaW1mX9pEmTlJWVpa997WvhjIUJMg4AAE+L5qWKxsbGiPXx8fHGNXs+KRgMdrk0MHDgQKWlpfU41yAYDMrn83XJEmRmZvbY5/XXX9fTTz+tjRs3htdlZWVp1apVmjx5slpbW/X444/r+uuv144dO3T11Vc7PoZ+GzicOXPGqHCTTfEZ0z42RaFsxuXz+Yz72PD7/cZ9TIvc2KitrY36PiS7z8amMNaJEyeM2tsULLMZ15AhQ4z72BQtGzBggHEfm6JNpmyKb7nF9Ny0KVjW1tZm3Me0MJhNIbH+JicnJ+LnpUuX6r777uvS7u6779ZPf/rT825r3759vTm0Hu3Zs0c33XSTli5dqunTp4fXjx07NqLo2Je+9CUdPHhQy5cv1//5P//H8fb7beAAAIAT0cw41NbWRkwk7CnbsHjxYs2fP/+82xwzZoz8fr+OHz8esf7s2bM6efJkj3/M+f1+tbW1qb6+PiLrUFdX16XP22+/rYKCAi1cuFD33HPPeccjSVOmTNGf//znC7b7JAIHAAB6kJycHBE49CQ9PV3p6ekXbBcIBFRfX6+qqirl5eVJkrZs2aKOjg7l5+d32ycvL09xcXHavHmziouLJX18Z8Thw4cVCATC7fbu3atp06Zp3rx5euCBB5wcnnbt2qWsrCxHbTsROAAAPM1Lt2Pm5uZqxowZWrBggVatWqX29naVlZVpzpw54Tsqjhw5ooKCAv3ud7/TlClTlJKSopKSEpWXlystLU3Jycm64447FAgENHXqVEkfX56YNm2aioqKVF5eHp77MGDAgHBA89BDD2n06NG64oor1NLSoscff1xbtmzRSy+9ZHQMBA4AALho7dq1KisrU0FBgWJjY1VcXKxHHnkk/Hp7e7uqq6t1+vTp8Lrly5eH27a2tqqoqEgrV64Mv/7HP/5RJ06c0JNPPqknn3wyvH7kyJE6dOiQpI/ntSxevFhHjhzR4MGDNWHCBL388su64YYbjMYfE+pnz9VsbGxUSkqKqqurlZSU5LifzUQ3m8mOpmzGZTPRycanJ/04YTo50mYCVkJCgnEfG25NjjSdhHexTY60mYT4eZ8caXoO2Eyobm5uNu5j+p1pamrS+PHj1dDQ4Cjdb6rz34tLL73UahLu+Zw7d04HDhyI2ti9jOc4AAAAx7hUAQDwNC/NcbgYEDgAADyNwMFdXKoAAACOkXEAAHgeGQL3kHEAAACO9duMg8/n+0yFRJywuYXNlM1tZTa3idrcwmhTE6KlpcW4jym3bt+z2Y/Ns/dN/xKyOS9t/tqyue2zv9YesLkVz63bcd3Yj83vSpv3bOTIkUbt3coCMMfBXWQcAACAY/024wAAgBNkHNxFxgEAADhGxgEA4GlkHNxF4AAA8DQCB3dxqQIAADhGxgEA4GlkHNxFxgEAADhGxgEA4GlkHNxFxgEAADhGxgEA4GlkHNxFxgEAADh20WQc2trajPvExcUZtbcpPpWYmGjc569//atxH5uxuRFR2xTSsSmk5PP5jPucPn3auI9NwSLT9zknJ8d4H4cOHTLuY/M+2xy/6fdMMj+fbcZlc27afGdsiqmZvmenTp0y3odNYayamhqj9k1NTRo/frzxfkyRcXDXRRM4AAA+nwgc3GV0qeKxxx7ThAkTlJycrOTkZAUCAf3Xf/1X+PWWlhaVlpZq6NChSkxMVHFxserq6np90AAAoG8YBQ7Dhw/Xf/zHf6iqqkpvvvmmpk2bpptuukl79+6VJC1atEjPP/+81q9fr23btuno0aOaPXt2VAYOAID0PxmH3l7QPaNLFd/4xjcifn7ggQf02GOPafv27Ro+fLieeOIJrVu3TtOmTZMkrV69Wrm5udq+fbumTp3ae6MGAAB9wvquinPnzumpp55Sc3OzAoGAqqqq1N7ersLCwnCbcePGacSIEaqsrOyVwQIA8GlkHNxlPDny//2//6dAIKCWlhYlJibq2Wef1fjx47Vr1y75fD6lpqZGtM/MzFQwGOxxe62trWptbQ3/3NjYaDokAADgEuPAYezYsdq1a5caGhr0xz/+UfPmzdO2bdusB7Bs2TLdf//91v0BAJ9v3FXhLuNLFT6fT5deeqny8vK0bNkyTZw4UQ8//LD8fr/a2tpUX18f0b6urk5+v7/H7S1ZskQNDQ3hpba21vggAACAOz7zkyM7OjrU2tqqvLw8xcXFafPmzeHXqqurdfjwYQUCgR77x8fHh2/v7FwAAHCKOQ7uMrpUsWTJEs2cOVMjRoxQU1OT1q1bp61bt+rFF19USkqKSkpKVF5errS0NCUnJ+uOO+5QIBDgjgoAQNRwqcJdRoHD8ePH9Q//8A86duyYUlJSNGHCBL344ov62te+Jklavny5YmNjVVxcrNbWVhUVFWnlypVRGTgAAHCfUeDwxBNPnPf1hIQErVixQitWrPhMgwIAwCkyDu7qt7Uq2traIm7TvJCBA80PpaWlxai9TVEct4rP2HCjyJFbXz6bY7H5PG2KKcXGmk0lOnr0qPE+bMZlUxjN5tw0/Z5J5t9n0/dY+vhJuKbee+894z5uFNMaPHiw8T5siryZjsumwBf6v34bOAAA4AQZB3d95rsqAADA5wcZBwCAp5FxcBcZBwAA4BgZBwCA55EhcA8ZBwAA4BgZBwCAp0Uj20AGo2cEDgAATyNwcBeXKgAAgGNkHAAAnkbGwV1kHAAAgGP9LuPQGeWZ1njor7UqbNg8d9/G2bNnjfuYPnvepoaCjba2NuM+Nsdvczymf7nY7MOmj1vnc3+tVWHzF2VTU5NxHzdqosTFxRnvw41aFZ2/x6P91zsZB3f1u8Ch84uZl5fXxyMBgEjjx4/v6yF4UlNTk1JSUvp6GOgl/S5wyM7OVm1trZKSkrpE3Y2NjcrJyVFtba2Sk5P7aIR9h+Pn+Dl+jt9Lxx8KhdTU1KTs7Oyo78cL27xY9LvAITY29oLlbpOTkz3zxYkGjp/j5/g5fq8g03Dx6XeBAwAAJsg4uIvAAQDgaQQO7vLU7Zjx8fFaunSp4uPj+3oofYLj5/g5fo7/83r86D9iQoRVAAAPamxsVEpKigYNGtTrt36HQiGdOXNGDQ0NnppT4gZPZRwAAEDfYo4DAMDTmOPgLjIOAAC46OTJk5o7d66Sk5OVmpqqkpKSCz4tuaWlRaWlpRo6dKgSExNVXFysurq68OsfffSRZsyYoezsbMXHxysnJ0dlZWVqbGyM2M7WrVt19dVXKz4+XpdeeqnWrFljPH4CBwCAp4VCoags0TJ37lzt3btXFRUV2rBhg1599VUtXLjwvH0WLVqk559/XuvXr9e2bdt09OhRzZ49O/x6bGysbrrpJv3nf/6n3nnnHa1Zs0Yvv/yybr/99nCbmpoa3Xjjjbrhhhu0a9cu3Xnnnfrud7+rF1980ewAQh7xq1/9KjRy5MhQfHx8aMqUKaEdO3b09ZBcs3Tp0pCkiGXs2LF9Payo2bZtW+jrX/96KCsrKyQp9Oyzz0a83tHREfrRj34U8vv9oYSEhFBBQUHonXfe6ZvBRsGFjn/evHldzoeioqK+GWwU/Pu//3to8uTJocTExFB6enropptuCu3fvz+izZkzZ0L/63/9r1BaWlpoyJAhodmzZ4eCwWAfjbh3OTn+r371q13OgX/8x3/soxH3nYaGhpCkUHx8fCghIaFXl/j4+JCkUENDQ6+O+e233w5JCr3xxhvhdf/1X/8ViomJCR05cqTbPvX19aG4uLjQ+vXrw+v27dsXkhSqrKzscV8PP/xwaPjw4eGf77rrrtAVV1wR0eaWW24x/v3hiYzD008/rfLyci1dulRvvfWWJk6cqKKiIh0/fryvh+aaK664QseOHQsvf/7zn/t6SFHT3NysiRMnasWKFd2+/uCDD+qRRx7RqlWrtGPHDg0ZMkRFRUVWxZT6owsdvyTNmDEj4nz4/e9/7+IIo2vbtm0qLS3V9u3bVVFRofb2dk2fPl3Nzc3hNhf668vLnBy/JC1YsCDiHHjwwQf7aMR9LxTFjENjY2PE0tra+pnGWllZqdTUVE2ePDm8rrCwULGxsdqxY0e3faqqqtTe3q7CwsLwunHjxmnEiBGqrKzsts/Ro0f1zDPP6Ktf/WrEvj+5DUkqKirqcRs9Mgoz+siUKVNCpaWl4Z/PnTsXys7ODi1btqwPR+WepUuXhiZOnNjXw+gT+tRf3B0dHSG/3x/62c9+Fl5XX18fio+PD/3+97/vgxFG16ePPxT6OONw00039cl4+sLx48dDkkLbtm0LhUL2f3151aePPxT6OOPwz//8z303qH6iM+MQFxcX8vl8vbrExcV1yepICi1duvQzjfmBBx4IXX755V3Wp6enh1auXNltn7Vr14Z8Pl+X9ddcc03orrvuilg3Z86c0KBBg0KSQt/4xjdCZ86cCb922WWXhf793/89ov3GjRtDkkKnT592fAz9PuPQ1tamqqqqiCgpNjZWhYWF5lGSh7377rvKzs7WmDFjNHfuXB0+fLivh9QnampqFAwGI86HlJQU5efnf67Oh61btyojI0Njx47V9773PX300Ud9PaSoaWhokCSlpaVJsvvry8s+ffyd1q5dq2HDhunKK6/UkiVLrMpk48Jqa2vV0NAQXpYsWdJtu7vvvlsxMTHnXfbv3x/18S5fvlxvvfWWnnvuOR08eFDl5eW9vo9+fzvmhx9+qHPnzikzMzNifWZmpisfQn+Qn5+vNWvWaOzYsTp27Jjuv/9+feUrX9GePXuUlJTU18NzVTAYlKRuz4fO1y52M2bM0OzZszV69GgdPHhQP/zhDzVz5kxVVlZqwIABfT28XtXR0aE777xTX/7yl3XllVdK+vgc8Pl8Sk1NjWh7MZ4D3R2/JH3729/WyJEjlZ2drd27d+sHP/iBqqur9cwzz/ThaPtOKIq3YzotKrZ48WLNnz//vG3GjBkjv9/f5TL72bNndfLkSfn9/m77+f1+tbW1qb6+PuK8r6ur69LH7/fL7/dr3LhxSktL01e+8hX96Ec/UlZWlvx+f8SdGJ3bSE5O1qBBgy54jJ36feAAaebMmeH/nzBhgvLz8zVy5Ej94Q9/UElJSR+ODH1hzpw54f+/6qqrNGHCBF1yySXaunWrCgoK+nBkva+0tFR79uy5qOf0nE9Px//JGfhXXXWVsrKyVFBQoIMHD+qSSy5xe5iQlJ6ervT09Au2CwQCqq+vV1VVlfLy8iRJW7ZsUUdHh/Lz87vtk5eXp7i4OG3evFnFxcWSpOrqah0+fFiBQKDHfXV0dEhSeF5GIBDQCy+8ENGmoqLivNvoTr+/VDFs2DANGDCg2yipp+jsYpeamqrLL79cBw4c6OuhuK7zM+d8+B9jxozRsGHDLrrzoaysTBs2bNArr7yi4cOHh9d/8q+vT7rYzoGejr87nf/gXGzngFMhD92OmZubqxkzZmjBggXauXOnXnvtNZWVlWnOnDnKzs6WJB05ckTjxo3Tzp07JX18ObakpETl5eV65ZVXVFVVpdtuu02BQEBTp06VJL3wwgtavXq19uzZo0OHDmnjxo26/fbb9eUvf1mjRo2SJN1+++167733dNddd2n//v1auXKl/vCHP2jRokVGx9DvAwefz6e8vDxt3rw5vK6jo0ObN282jpIuFqdOndLBgweVlZXV10Nx3ejRo+X3+yPOh8bGRu3YseNzez588MEH+uijjy6a8yEUCqmsrEzPPvustmzZotGjR0e8/sm/vjo5+evLKy50/N3ZtWuXJF0058DFbu3atRo3bpwKCgo0a9YsXXvttfr1r38dfr29vV3V1dUR81aWL1+ur3/96youLtZ1110nv98fcWlq0KBB+t//+3/r2muvVW5urhYtWqRvfvOb2rBhQ7jN6NGjtXHjRlVUVGjixIn6xS9+occff1xFRUVmB+B4GmUfeuqpp0Lx8fGhNWvWhN5+++3QwoULQ6mpqRfNfdsXsnjx4tDWrVtDNTU1oddeey1UWFgYGjZsWOj48eN9PbSoaGpqCv3lL38J/eUvfwlJCv3yl78M/eUvfwm9//77oVAoFPqP//iPUGpqaui5554L7d69O3TTTTeFRo8eHTF72MvOd/xNTU2h73//+6HKyspQTU1N6OWXXw5dffXVocsuuyzU0tLS10PvFd/73vdCKSkpoa1bt4aOHTsWXj456/v2228PjRgxIrRly5bQm2++GQoEAqFAINCHo+49Fzr+AwcOhH784x+H3nzzzVBNTU3oueeeC40ZMyZ03XXX9fHI3dd5V0VsbGxowIABvbrExsZG5TkOFwNPBA6hUCj06KOPhkaMGBHy+XyhKVOmhLZv397XQ3LNLbfcEsrKygr5fL7Q3/zN34RuueWW0IEDB/p6WFHzyiuvdHsb1Lx580Kh0P88ACozMzMUHx8fKigoCFVXV/ftoHvR+Y7/9OnToenTp4fS09NDcXFxoZEjR4YWLFhwUQXR3R27pNDq1avDbTofAPWFL3whNHjw4NDf/u3fho4dO9Z3g+5FFzr+w4cPh6677rpQWlpaKD4+PnTppZeG/uVf/uVz+Q8cgUPfoKw2AMCTOstqd97u2JtC//88B8pqd8VdFQAAT4vG37/8Td2zfj85EgAA9B9kHAAAnkeGwD1kHAAAgGMEDgAAT/L5fFF96Jff75fP54va9r2KuyoAAJ7V0tKitra2qGzb5/MpISEhKtv2MgIHAADgGJcqAACAYwQOAADAMQIHAADgGIEDAABwjMABAAA4RuAAAAAcI3AAAACO/X9+zS0/GIjbrAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f8090d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(3.5763e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# backprop through batchnorm but all in one go\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57a79bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "\n",
    "# -----------------\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw* (dhpreact*bnraw).sum(0))\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b00692e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12719\n",
      "      0/ 200000: 3.8111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000/ 200000: 1.5936\n",
      "  20000/ 200000: 2.1010\n",
      "  30000/ 200000: 1.4711\n",
      "  40000/ 200000: 2.1248\n",
      "  50000/ 200000: 1.6181\n",
      "  60000/ 200000: 1.7196\n",
      "  70000/ 200000: 1.2026\n",
      "  80000/ 200000: 1.3953\n",
      "  90000/ 200000: 1.4356\n",
      " 100000/ 200000: 1.4518\n",
      " 110000/ 200000: 1.2661\n",
      " 120000/ 200000: 1.2207\n",
      " 130000/ 200000: 1.4259\n",
      " 140000/ 200000: 1.5294\n",
      " 150000/ 200000: 1.5008\n",
      " 160000/ 200000: 1.4570\n",
      " 170000/ 200000: 1.6469\n",
      " 180000/ 200000: 1.1593\n",
      " 190000/ 200000: 1.3232\n"
     ]
    }
   ],
   "source": [
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * CONTEXT_SIZE, n_hidden), generator=g) * (5/3)/((n_embd * CONTEXT_SIZE)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = X_train[ix], Y_train[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "  #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1ba2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[X_train]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "042f6632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.3426601886749268\n",
      "val 1.3950341939926147\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (X_train, Y_train),\n",
    "    'val': (X_dev, Y_dev),\n",
    "    'test': (X_test, Y_test),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
