{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road to Generative AI - Part 2: Multi-Layer Perceptron\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this notebook is to explore the capabilities of Generative AI. In the [first part](https://tdody.github.io//bigram-and-nn/) of this series, we explored the concept of Generative AI and built a simple model that generates text using a bigram model and a single-layer NN. In this part, we will build a more complex model using Multi-Layer Perceptron (MLP) to generate text.\n",
    "\n",
    "The reason behind using MLP is that N-gram models suffer from the curse of dimensionality. As the size of the n-gram increases, the number of possible n-grams grows exponentially. This makes it difficult to store and process the n-grams. In contrast, MLP can learn the patterns in the data and generate text without the need to store all possible n-grams.\n",
    "\n",
    "In the paper \"A Neural Probabilistic Language Model\" by Yoshua Bengio et al., the authors proposed a neural network-based language model that can learn to predict the next word in a sentence. We are going to use a similar approach but our focus is on generating the next character in a sequence of characters.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use a dataset containing common bird names. Our source data can be found [here](https://www.kaggle.com/datasets/thepushkarp/common-bird-names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 birds in the dataset:\n",
      "Abbott's babbler, Abbott's booby, Abbott's starling, Abbott's sunbird, Abd al-Kuri sparrow, Abdim's stork, Aberdare cisticola, Aberrant bush warbler, Abert's towhee, Abyssinian catbird\n",
      "There are 10,976 birds in the dataset.\n",
      "The shortest character name has 3 birds.\n",
      "The longest character name has 35 birds.\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"./datasets/birds/birds.csv\"\n",
    "\n",
    "birds = open(DATASET_PATH, \"r\").read().splitlines()\n",
    "\n",
    "print(\"First 10 birds in the dataset:\")\n",
    "print(\", \".join(birds[:10]))\n",
    "print(f\"There are {len(birds):,d} birds in the dataset.\")\n",
    "\n",
    "min_length = map(len, birds)\n",
    "max_length = map(len, birds)\n",
    "print(f\"The shortest character name has {min(min_length)} birds.\")\n",
    "print(f\"The longest character name has {max(max_length)} birds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def clean_name(name):\n",
    "    # Remove leading and trailing whitespaces\n",
    "    # Convert to lowercase\n",
    "    # Remove accents\n",
    "    # Remove special characters\n",
    "    # Replace spaces with underscores\n",
    "\n",
    "    name = name.strip().lower()\n",
    "    # replace special characters with a space\n",
    "    name = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in name)\n",
    "    name = name.replace(\"`\", \"_\")  # Remove apostrophes\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    name = unidecode(name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning process:\n",
      "Original: collared_brushturkey -> Cleaned: collared_brushturkey\n",
      "Original: eurasian_coot -> Cleaned: eurasian_coot\n",
      "Original: shoebill -> Cleaned: shoebill\n",
      "Original: mountain_leaf_warbler -> Cleaned: mountain_leaf_warbler\n",
      "Original: hood_mockingbird -> Cleaned: hood_mockingbird\n",
      "Original: black_browed_triller -> Cleaned: black_browed_triller\n",
      "Original: mascarene_parrot -> Cleaned: mascarene_parrot\n",
      "Original: yellow_bellied_wattle_eye -> Cleaned: yellow_bellied_wattle_eye\n",
      "Original: black_shouldered_cicadabird -> Cleaned: black_shouldered_cicadabird\n",
      "Original: white_headed_vulture -> Cleaned: white_headed_vulture\n"
     ]
    }
   ],
   "source": [
    "# clean a few random names from the dataset\n",
    "import numpy as np\n",
    "rdm_indexes = np.random.randint(0, len(birds), 10)\n",
    "\n",
    "print(\"Cleaning process:\")\n",
    "for i in rdm_indexes:\n",
    "    name = birds[i]\n",
    "    cleaned_name = clean_name(name)\n",
    "    print(f\"Original: {name} -> Cleaned: {cleaned_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 27\n",
      "_, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z\n",
      "Token mapping: {1: 'j', 2: 'p', 3: 'x', 4: '_', 5: 'd', 6: 'r', 7: 'q', 8: 'm', 9: 'o', 10: 'i', 11: 'z', 12: 'u', 13: 'h', 14: 'y', 15: 't', 16: 'l', 17: 'e', 18: 'a', 19: 'c', 20: 'k', 21: 'g', 22: 's', 23: 'b', 24: 'w', 25: 'v', 26: 'n', 27: 'f', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# clean all names in the dataset\n",
    "birds = list(map(clean_name, birds))\n",
    "\n",
    "# create a mapping from tokens to indices\n",
    "unique_tokens = set([c for w in birds for c in w])\n",
    "SPECIAL_TOKEN = \".\"\n",
    "index_to_token = {i: t for i, t in enumerate(unique_tokens, start=1)}\n",
    "token_to_index = {v: k for k, v in index_to_token.items()}\n",
    "index_to_token[0] = SPECIAL_TOKEN\n",
    "token_to_index[SPECIAL_TOKEN] = 0\n",
    "\n",
    "# log information about the tokenization\n",
    "print(f\"Number of unique tokens: {len(unique_tokens)}\")\n",
    "print(\", \".join(sorted(unique_tokens)))\n",
    "print(f\"Token mapping: {index_to_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The main limitation of the bigram model is its scalability. As we increase the context size, the number of unique n-grams grows exponentially, making it difficult to store and process them. To overcome this limitation, we will use a Multi-Layer Perceptron (MLP) model.\n",
    "\n",
    "The figure below shows the architecture of the MLP model. The model consists of an embedding layer $C$. The embeddings are fed into a hidden layer $H$ with a $tanh$ activation function. The output of the hidden layer is fed into the output layer $O$ with a softmax activation function. The output layer generates the probability distribution of the next character in the sequence.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/MLP_architecture.png\" width=\"500\"/>\n",
    "    <figcaption>Neural architecture</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we create a sample training set for a single bird name. The input sequence ($X$) is the context containing 3 characters, and the target sequence ($Y$) is the next character to predict. The model will learn to predict the next character based on the context.\n",
    "\n",
    "For the first character, the context is empty, so we use a special token `.` to indicate the start of the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbott_s_babbler\n",
      "... -> a\n",
      "..a -> b\n",
      ".ab -> b\n",
      "abb -> o\n",
      "bbo -> t\n",
      "bot -> t\n",
      "ott -> _\n",
      "tt_ -> s\n",
      "t_s -> _\n",
      "_s_ -> b\n",
      "s_b -> a\n",
      "_ba -> b\n",
      "bab -> b\n",
      "abb -> l\n",
      "bbl -> e\n",
      "ble -> r\n",
      "ler -> .\n",
      "abbott_s_booby\n",
      "... -> a\n",
      "..a -> b\n",
      ".ab -> b\n",
      "abb -> o\n",
      "bbo -> t\n",
      "bot -> t\n",
      "ott -> _\n",
      "tt_ -> s\n",
      "t_s -> _\n",
      "_s_ -> b\n",
      "s_b -> o\n",
      "_bo -> o\n",
      "boo -> b\n",
      "oob -> y\n",
      "oby -> .\n",
      "abbott_s_starling\n",
      "... -> a\n",
      "..a -> b\n",
      ".ab -> b\n",
      "abb -> o\n",
      "bbo -> t\n",
      "bot -> t\n",
      "ott -> _\n",
      "tt_ -> s\n",
      "t_s -> _\n",
      "_s_ -> s\n",
      "s_s -> t\n",
      "_st -> a\n",
      "sta -> r\n",
      "tar -> l\n",
      "arl -> i\n",
      "rli -> n\n",
      "lin -> g\n",
      "ing -> .\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "X, Y = [], []\n",
    "\n",
    "for i, bird in enumerate(birds[0:100]):\n",
    "    if i<3:\n",
    "        print(bird)\n",
    "    context = [0] * CONTEXT_SIZE\n",
    "    for ch in bird + SPECIAL_TOKEN:  # Add special token at the end\n",
    "        ix = token_to_index[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        if i < 3:\n",
    "            print(''.join([index_to_token[i] for i in context]), '->', index_to_token[ix])\n",
    "        # Update the context by shifting it and adding the new index \n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.int64)\n",
    "Y = torch.tensor(Y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "X shape: torch.Size([1915, 3])\n",
      "Y shape: torch.Size([1915])\n",
      "\n",
      "First 10 examples:\n",
      "X: tensor([[ 0,  0,  0],\n",
      "        [ 0,  0, 18],\n",
      "        [ 0, 18, 23],\n",
      "        [18, 23, 23],\n",
      "        [23, 23,  9],\n",
      "        [23,  9, 15],\n",
      "        [ 9, 15, 15],\n",
      "        [15, 15,  4],\n",
      "        [15,  4, 22],\n",
      "        [ 4, 22,  4]])\n",
      "Y: tensor([18, 23, 23,  9, 15, 15,  4, 22,  4, 23])\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset information:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "\n",
    "print(\"\\nFirst 10 examples:\")\n",
    "print(\"X:\", X[0:10])\n",
    "print(\"Y:\", Y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on the embedding process. The embedding layer is a matrix that maps each token to a vector of fixed size. The size of the vector is called the embedding dimension. The embedding layer is initialized with random values and is trained during the training process. Below we create a random embedding matrix for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_token = len(unique_tokens)+1\n",
    "EMBEDDING_DIM = 2\n",
    "\n",
    "C = torch.randn((n_token, EMBEDDING_DIM), dtype=torch.float32) # shape (28, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to retrieve the embedding for a specific token, we can use the token's index to look up the corresponding row in the embedding matrix. This allows us to convert tokens into their vector representations, which can then be used as input to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6547,  1.0788])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_embed = \"t\"\n",
    "token_index = token_to_index[token_to_embed]\n",
    "one_hot_encoded = F.one_hot(torch.tensor(token_index), num_classes=n_token).float() # shape (28)\n",
    "\n",
    "\n",
    "# multiply the one-hot encoded vector with the embedding matrix\n",
    "one_hot_encoded @ C # (28, 1) @ (28, EMBEDDING_DIM) -> (1, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings of X:  torch.Size([1915, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can use Pytorch indexing to get the embedding for a specific token\n",
    "print(\"Shape of embeddings of X: \", C[X].shape) # (n_examples, context_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage of the model, the embedding layer took care of transforming the input tokens into their vector representations. However, the context shape (`(n_examples, context_size, embedding_size)`) is not compatible with the input shape of the MLP model. To make it compatible, we need to flatten the context shape into a single vector for each example so the input shape becomes `(n_examples, context_size * embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the flattened context: torch.Size([1915, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the flattened context:\", C[X].view((-1, EMBEDDING_DIM * CONTEXT_SIZE)).shape) # flatten the context shape into a single vector for each example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the right input shape, we can now build the MLP model. The model consists of an embedding layer, a hidden layer with a $tanh$ activation function, and an output layer with a softmax activation function. The output layer generates the probability distribution of the next character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_SIZE = 128\n",
    "W1 = torch.randn((EMBEDDING_DIM * CONTEXT_SIZE, LAYER_SIZE), dtype=torch.float32) # shape (30, 128)\n",
    "b1 = torch.randn((LAYER_SIZE,), dtype=torch.float32) # shape (128,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output probabilities: torch.Size([1915, 28])\n"
     ]
    }
   ],
   "source": [
    "# Hidden layer\n",
    "# Note: the '+' relies on broadcasting, so the bias is added to each row of the matrix\n",
    "# (n_examples, LAYER_SIZE) + (LAYER_SIZE,) -> (n_examples, LAYER_SIZE)\n",
    "h = torch.tanh(C[X].view((-1, EMBEDDING_DIM * CONTEXT_SIZE)) @ W1 + b1) # shape (n_examples, LAYER_SIZE)\n",
    "\n",
    "# Output layer interms of weights and biases\n",
    "W2 = torch.randn((LAYER_SIZE, n_token), dtype=torch.float32) # shape (128, 28)\n",
    "b2 = torch.randn((n_token,), dtype=torch.float32) # shape (28,)\n",
    "\n",
    "# Output layer\n",
    "y = h @ W2 + b2\n",
    "# Apply softmax to get the probability distribution of the next character\n",
    "probs = F.softmax(y, dim=1) # shape (n_examples, n_token)\n",
    "\n",
    "print(\"Shape of the output probabilities:\", probs.shape) # (n_examples, n_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the loss function. The loss function measures how well the model predicts the next character in the sequence. We will use the cross-entropy loss function, which is commonly used for classification tasks. The cross-entropy loss function compares the predicted probability distribution with the true distribution and computes the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 19.839637756347656\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(y, Y) # compute the cross-entropy loss\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean things up a bit and set up the training loop. The training loop will iterate over the dataset, compute the loss, and update the model parameters using backpropagation. We will use the Adam optimizer to update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(1234)\n",
    "\n",
    "C = torch.randn((n_token, EMBEDDING_DIM), dtype=torch.float32, generator=g) # shape (28, 10)\n",
    "W1 = torch.randn((EMBEDDING_DIM * CONTEXT_SIZE, LAYER_SIZE), dtype=torch.float32, generator=g) # shape (30, 128)\n",
    "b1 = torch.randn((LAYER_SIZE,), dtype=torch.float32, generator=g) # shape (128,)\n",
    "W2 = torch.randn((LAYER_SIZE, n_token), dtype=torch.float32, generator=g) # shape (128, 28)\n",
    "b2 = torch.randn((n_token,), dtype=torch.float32, generator=g) # shape (28,)\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True  # Set requires_grad to True to enable backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 0: 2.0080769062042236\n",
      "Loss at 100: 2.0022177696228027\n",
      "Loss at 200: 1.996519684791565\n",
      "Loss at 300: 1.9909712076187134\n",
      "Loss at 400: 1.9855639934539795\n",
      "Loss at 500: 1.9802888631820679\n",
      "Loss at 600: 1.9751393795013428\n",
      "Loss at 700: 1.9701075553894043\n",
      "Loss at 800: 1.9651888608932495\n",
      "Loss at 900: 1.9603769779205322\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    # Forward pass\n",
    "    h = torch.tanh(C[X].view((-1, EMBEDDING_DIM * CONTEXT_SIZE)) @ W1 + b1)  # shape (n_examples, LAYER_SIZE)\n",
    "    y = h @ W2 + b2  # shape (n_examples, n_token)\n",
    "    loss = F.cross_entropy(y, Y)  # compute the cross-entropy loss\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Loss at {i}: {loss.item()}\")  # Print the loss value for monitoring\n",
    "\n",
    "    # Backward pass\n",
    "    for p in params:\n",
    "        p.grad = None  # Reset gradients to zero before backpropagation\n",
    "    loss.backward()  # Compute gradients\n",
    "\n",
    "    # Update parameters using gradient descent\n",
    "    for p in params:\n",
    "        p.data -= 0.01 * p.grad  # Update parameters with a learning rate of 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res. 3, null (3/1/2003), 1137–1155."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_bot-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
