{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road to Generative AI - Part 1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this notebook is to explore the capabilities of Generative AI. In this first part, we will focus on the most basic form of Generative AI, which is the generation of simple words. We will use a series of model architectures from simple bigram models to more complex RNNs and LSTMs to generate new words based on a dataset of existing words.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this task, we will build a model that will help us generate new names for the Lord of the Rings universe. We will use the names of the characters in the Lord of the Rings series as our dataset. Our source data can be found [here](https://www.kaggle.com/paultimothymooney/lord-of-the-rings-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adanel', 'Boromir', 'Lagduf', 'Tarcil', 'Fire-drake of Gondolin', 'Ar-Adûnakhôr', 'Annael', 'Angrod', 'Angrim', 'Anárion']\n",
      "There are 909 characters in the dataset.\n",
      "The shortest character name has 2 characters.\n",
      "The longest character name has 31 characters.\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"./datasets/lotr/lotr_characters_names.csv\"\n",
    "\n",
    "characters = open(DATASET_PATH, \"r\").read().splitlines()\n",
    "\n",
    "print(characters[0:10])\n",
    "print(f\"There are {len(characters)} characters in the dataset.\")\n",
    "\n",
    "min_length = map(len, characters)\n",
    "max_length = map(len, characters)\n",
    "print(f\"The shortest character name has {min(min_length)} characters.\")\n",
    "print(f\"The longest character name has {max(max_length)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adanel', 'boromir', 'lagduf', 'tarcil', 'fire-drake_of_gondolin', 'ar-adunakhor', 'annael', 'angrod', 'angrim', 'anarion']\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def clean_name(name):\n",
    "    # Remove leading and trailing whitespaces\n",
    "    # Convert to lowercase\n",
    "    # Remove accents\n",
    "    # Remove special characters\n",
    "    # Replace spaces with underscores\n",
    "\n",
    "    name = name.strip().lower()\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    name = unidecode(name)\n",
    "    return name\n",
    "\n",
    "characters = list(map(clean_name, characters))\n",
    "\n",
    "print(characters[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n",
    "\n",
    "The simplest form of Generative AI is the bigram model. In this model, we calculate the probability of each character in the dataset based on the previous character. We then use these probabilities to generate new characters. Let's start by building a bigram model for our dataset.\n",
    "\n",
    "A language model leads the conditional probailities $P(c_i|c_{i-1})$ where $c_i$ is the $i$-th character in the dataset. We can calculate these probabilities by counting the number of times each character appears after the previous character in the dataset. Then at prediction time, we can generate new characters by sampling from the conditional probabilities.\n",
    "\n",
    "For the name \"adanel\", the model needs to understand that the first \"a\" is the first character and that the \"l\" is the last character. To do so, we are going to add a start token `<START>` at the beginning of each name and an end token `<END>` at the end of each name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> a\n",
      "a d\n",
      "d a\n",
      "a n\n",
      "n e\n",
      "e l\n",
      "l <END>\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "\n",
    "for w in characters[0:1]:\n",
    "    w = [START_TOKEN] + list(w) + [END_TOKEN]\n",
    "    for c1, c2 in zip(w, w[1:]):\n",
    "        print(c1, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate a dictionary that contains the conditional probabilities for each character in the dataset. We will use this dictionary to generate new names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<START>', 'a') 101\n",
      "('a', 'd') 53\n",
      "('d', 'a') 56\n",
      "('a', 'n') 184\n",
      "('n', 'e') 44\n"
     ]
    }
   ],
   "source": [
    "bigrams = {}\n",
    "\n",
    "for w in characters:\n",
    "    w = [START_TOKEN] + list(w) + [END_TOKEN]\n",
    "    for c1, c2 in zip(w, w[1:]):\n",
    "        bigram = (c1, c2)\n",
    "        bigrams[bigram] = bigrams.get(bigram, 0) + 1\n",
    "\n",
    "# print a sample of 5 bigrams\n",
    "for c1 in list(bigrams.keys())[0:5]:\n",
    "    print(c1, bigrams[c1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 465 bigrams in the dataset.\n",
      "The most common bigrams are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('a', 'r'), 220),\n",
       " (('i', 'n'), 211),\n",
       " (('r', '<END>'), 188),\n",
       " (('a', 'n'), 184),\n",
       " (('o', 'r'), 170)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"There are {len(bigrams)} bigrams in the dataset.\")\n",
    "print(\"The most common bigrams are:\")\n",
    "sorted(bigrams.items(), key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 unique tokens in the dataset.\n",
      "{'-', 'm', 'b', 'p', 'y', 'n', 'a', '_', 'x', 'w', 'd', 'q', 'j', 'l', '.', 'g', 'i', 'f', 'u', 'c', 'o', 'v', 't', 'h', 'k', \"'\", 'z', 'r', 'e', 's'}\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set([c for w in characters for c in w])\n",
    "print(f\"There are {len(unique_tokens)} unique tokens in the dataset.\")\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, we are going to use a 2D Pytorch tensor to store the conditional probabilities. The first dimension will represent the previous character, and the second dimension will represent the current character. The value at index `[i, j]` will represent the probability of character `j` appearing after character `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# create our bigram array\n",
    "N = torch.zeroes(len(unique_tokens), len(unique_tokens), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Lord of the Rings Data](https://www.kaggle.com/paultimothymooney/lord-of-the-rings-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
